# -*- coding: utf-8 -*-
"""Laboratorio_04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h2ODQ0eQ1vhg7ThZIq8WrA-PVBDLds_H

#Clasificación multiclase
Taquichiri Huarita Luis Alexander

Importar librerías
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
from matplotlib import pyplot
from scipy import optimize

# %matplotlib inline

"""Cargar y revisar el data set

"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/machine learning/datasets/Player_Attributes.csv', delimiter=',', decimal='.')
data = data.dropna()
data.info()

"""Cargamos las variables x, y y m demás de tomar datos del mismo data set para predicciones"""

X = data.iloc[:16000, [5,9,10,11,12,13,14,15,16,17]]
y = data.iloc[:16000, 4]
m = y.size

X_predic = data.iloc[16000:20000, [5,9,10,11,12,13,14,15,16,17]]
y_predic = data.iloc[16000:20000, 4]
m_predic = y_predic.size

num_labels = 4
J_history = []

"""Convertir los datos de y en clases (0,1,2,3)
0 = 0-25; 1=25.1-50; 2=50.1-75; 3=75.1-100
0=mal jugador 1=jugador medio 2=jugador bueno 3=muy buen jugador
"""

import numpy as np
def convertir_clases(y):
  m = y.size
  aux_y = np.zeros(m)
  for i in range(m):
    if y.iloc[i] <= 25:
      aux_y[i] = 0
    elif y.iloc[i] <= 50:
      aux_y[i] = 1
    elif y.iloc[i] <= 75:
      aux_y[i] = 2
    else:
      aux_y[i] = 3
  return aux_y

print(y)
y = convertir_clases(y)
y_predic = convertir_clases(y_predic)
print(y)
print(y_predic)

"""Funcion que normaliza los datos"""

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

"""Normalización"""

X_norm, mu, sigma = featureNormalize(X)
X_predic_norm, mu_predic, sigma_predic = featureNormalize(X_predic)

"""Función zigmoide"""

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

"""Concatenamos el x0 para los datos de entrenamiento y los datos de prueba"""

X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)
X_predic = np.concatenate([np.ones((m_predic, 1)), X_predic_norm], axis=1)

"""Funcion que calcula el costo y las thetas con el modelo de regresion logistica"""

def lrCostFunction(theta, X, y, lambda_):

    if y.dtype == bool:
        y = y.astype(int)

    J = 0
    m = y.size
    grad = np.zeros(theta.shape)

    h = sigmoid(X.dot(theta.T))

    temp = theta
    temp[0] = 0

    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))
    J_history.append(J)

    grad = (1 / m) * (h - y).dot(X)
    grad = grad + (lambda_ / m) * temp

    return J , grad

"""Funcion para calcular todas las thetas es decir las diferentes thetas para cada clase"""

def oneVsAll(X, y, num_labels, lambda_):

    m, n = X.shape

    all_theta = np.zeros((num_labels, n))

    for c in np.arange(num_labels):
        J_history = []
        initial_theta = np.zeros(n)
        options = {'maxiter': 100000}
        res = optimize.minimize(lrCostFunction,
                                initial_theta,
                                (X, (y == c), lambda_),
                                jac=True,
                                method='CG',
                                options=options)
        all_theta[c] = res.x
    return all_theta

"""Calculo de las thetas y grafica, la cual cada pico representa el descenso del costo para cada clase"""

lambda_ = 0.003
all_theta = oneVsAll(X, y, num_labels, lambda_)
print(all_theta.shape)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print(all_theta)

"""Predicción One vs All"""

def predictOneVsAll(all_theta, X):
    m = X.shape[0];
    num_labels = all_theta.shape[0]

    p = np.zeros(m)
    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1)
    return p

"""Calculamos la efectividad con el mismo data set"""

pred = predictOneVsAll(all_theta, X)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))
print("------------------------------------------------------------------------")
print("Datos comparados:")
print(pred[:100])
print(y[:100])

"""Prueba de efectividad con los datos de Prueba"""

pred_prueba = predictOneVsAll(all_theta, X_predic)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred_prueba == y_predic) * 100))
print("------------------------------------------------------------------------")
print("Datos comparados:")
print(pred_prueba[:100])
print(y_predic[:100])